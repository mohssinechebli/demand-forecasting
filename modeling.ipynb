{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlflow pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import mlflow\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import VectorAssembler, Tokenizer, HashingTF, IDF, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder # in case we have compute ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "my_spark = SparkSession.builder.appName(\"Sales_Forecast\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 20:05:55 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: integer (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Week: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing sales data\n",
    "sales_data = my_spark.read.csv(\"Online Retail.csv\", \n",
    "                               header=True, \n",
    "                               inferSchema=True, \n",
    "                               sep=\",\")\n",
    "# Display the schema\n",
    "sales_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define renaming dictionary\n",
    "renaming_dict = {\n",
    "    \"InvoiceNo\": \"invoice_no\",\n",
    "    \"StockCode\": \"stock_code\",\n",
    "    \"Description\": \"description\",\n",
    "    \"Quantity\": \"quantity\",\n",
    "    \"UnitPrice\": \"unit_price\",\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"Country\": \"country\",\n",
    "    \"InvoiceDate\": \"invoice_date\",\n",
    "    \"Year\": \"year\",\n",
    "    \"Month\": \"month\",\n",
    "    \"Week\": \"week\",\n",
    "    \"Day\": \"day\",\n",
    "    \"DayOfWeek\": \"day_of_week\"\n",
    "}\n",
    "class RenameColumns(Transformer):\n",
    "    def __init__(self, renaming_dict):\n",
    "        super(RenameColumns, self).__init__()\n",
    "        self.renaming_dict = renaming_dict\n",
    "\n",
    "    def _transform(self, dataset: DataFrame) -> DataFrame:\n",
    "        return dataset.select([col(c).alias(self.renaming_dict.get(c, c)) for c in dataset.columns])\n",
    "class MeanEncoder(Transformer):\n",
    "    def __init__(self, inputCol=None, targetCol=None, outputCol=None):\n",
    "        super(MeanEncoder, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.targetCol = targetCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        # Calculate the mean of the target column for each unique value in the input column\n",
    "        encoding_df = df.groupBy(self.inputCol).agg(F.mean(self.targetCol).alias(self.outputCol))\n",
    "        # Join this mean encoding back to the original DataFrame\n",
    "        return df.join(encoding_df, on=self.inputCol, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# stage 1 : Create an instance of the RenameColumns transformer\n",
    "rename_transformer = RenameColumns(renaming_dict)\n",
    "\n",
    "# stage 2 : Create the country StringIndexer\n",
    "country_indexer = StringIndexer(inputCol=\"country\",\n",
    "                                outputCol=\"country_index\")\n",
    "\n",
    "# stage 3 : Create the country OneHotEncoder\n",
    "country_encoder = OneHotEncoder(inputCol=\"country_index\",\n",
    "                                outputCol=\"country_fact\")\n",
    "\n",
    "# stage 4 : Create the stock_code MeanEncoder\n",
    "mean_encoder = MeanEncoder(inputCol=\"stock_code\", \n",
    "                           targetCol=\"quantity\", \n",
    "                           outputCol=\"stock_code_mean\")\n",
    "\n",
    "# stage 5 : Create the descripotion Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"description\", \n",
    "                      outputCol=\"description_words\")\n",
    "\n",
    "# stage 6 : Create the HashingTF \n",
    "hashing_tf = HashingTF(inputCol=\"description_words\", \n",
    "                       outputCol=\"description_tf\", \n",
    "                       numFeatures=1000)\n",
    "# stage 7 : Create the IDF\n",
    "idf = IDF(inputCol=\"description_tf\", outputCol=\"description_tfidf\")\n",
    "\n",
    "# stage 8 : Create the day_of_week_encoder\n",
    "day_of_week_encoder = OneHotEncoder(inputCols=[\"day_of_week\"], outputCols=[\"day_of_week_encoded\"])\n",
    "\n",
    "# stage 9 : Create the month_encoder\n",
    "month_encoder = OneHotEncoder(inputCols=[\"month\"], outputCols=[\"month_encoded\"])\n",
    "\n",
    "# stage 10 : Make a VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"invoice_no\",\n",
    "                                           \"country_fact\", \n",
    "                                           \"unit_price\", \n",
    "                                           \"customer_id\", \n",
    "                                           \"year\", \n",
    "                                           \"month_encoded\",\n",
    "                                           \"week\",\n",
    "                                           \"day\",\n",
    "                                           \"day_of_week_encoded\",\n",
    "                                           \"stock_code_mean\",\n",
    "                                           \"description_tfidf\"], \n",
    "                                outputCol=\"features\")\n",
    "# Make the pipeline\n",
    "demand_pipe = Pipeline(stages=[rename_transformer,\n",
    "                               country_indexer, \n",
    "                               country_encoder,\n",
    "                               mean_encoder,\n",
    "                               tokenizer,\n",
    "                               hashing_tf,\n",
    "                               idf,\n",
    "                               day_of_week_encoder,\n",
    "                               month_encoder,\n",
    "                               vec_assembler])\n",
    "# Fit and transform your data with the pipeline\n",
    "pipeline_model = demand_pipe.fit(sales_data)\n",
    "transformed_data = pipeline_model.transform(sales_data)\n",
    "\n",
    "# Define the splitting date\n",
    "split_date = \"2011-09-25\"\n",
    "\n",
    "# Split the data into two sets based on the splitting date\n",
    "training = transformed_data.filter(transformed_data.invoice_date <= split_date)\n",
    "test = transformed_data.filter(transformed_data.invoice_date > split_date)\n",
    "\n",
    "# Create the RegressionEvaluator instances\n",
    "rmse_evaluator = RegressionEvaluator(labelCol=\"quantity\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "mean_quantity = sales_data.agg(F.mean(\"Quantity\").alias(\"mean_quantity\")).collect()[0]['mean_quantity']\n",
    "mae_evaluator = RegressionEvaluator(labelCol=\"quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"quantity\", predictionCol=\"prediction\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 20:06:25 WARN MemoryStore: Not enough space to cache rdd_74_1 in memory! (computed 152.2 MiB so far)\n",
      "24/10/26 20:06:25 WARN BlockManager: Persisting block rdd_74_1 to disk instead.\n",
      "24/10/26 20:06:27 WARN MemoryStore: Not enough space to cache rdd_74_0 in memory! (computed 348.8 MiB so far)\n",
      "24/10/26 20:06:27 WARN BlockManager: Persisting block rdd_74_0 to disk instead.\n",
      "24/10/26 20:06:34 WARN MemoryStore: Not enough space to cache rdd_74_0 in memory! (computed 98.1 MiB so far)\n",
      "24/10/26 20:06:39 WARN MemoryStore: Not enough space to cache rdd_74_0 in memory! (computed 98.1 MiB so far)\n",
      "2024/10/26 20:07:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "# ML Experiments\n",
    "mlflow.set_experiment(\"demand_forecasting_random_forest\")\n",
    "# the model parameters\n",
    "params = {\n",
    "    'maxDepth': 2,\n",
    "    'numTrees': 20,\n",
    "}\n",
    "run_name = f\"maxDepth_{params['maxDepth']}_numTrees_{params['numTrees']}\"\n",
    "# Track\n",
    "with mlflow.start_run(run_name=run_name): \n",
    "    # Train\n",
    "    rf_regressor = RandomForestRegressor(featuresCol=\"features\",\n",
    "                                        labelCol=\"quantity\",\n",
    "                                        maxDepth =params[\"maxDepth\"],\n",
    "                                        numTrees=params[\"numTrees\"])  \n",
    "    model = rf_regressor.fit(training)\n",
    "    # Test\n",
    "    test_results = model.transform(test)\n",
    "    rmse = rmse_evaluator.evaluate(test_results)\n",
    "    rrmse = 100*(rmse/mean_quantity)\n",
    "    r2 = r2_evaluator.evaluate(test_results)\n",
    "    # Log \n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"RRMSE\", rrmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    mlflow.spark.log_model(model, \"RandomForestModel\")\n",
    "    mlflow.log_params(params)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Finetuning Hyperparameters\\ngrid_regressor = ParamGridBuilder()         .addGrid(rf_regressor.maxDepth, [5, 15])         .addGrid(rf_regressor.numTrees, [20, 50])         .build()\\n# Create CrossValidator\\ncrossval_regressor = CrossValidator(estimator=rf_regressor,\\n                                     estimatorParamMaps=grid_regressor,\\n                                     evaluator=rmse_evaluator,\\n                                     numFolds=3)  # 3-fold cross-validation\\n# Fit the models \\nmodels = crossval_regressor.fit(training)\\n# Extract the best model that has the best validation\\nmodel = models.bestModel\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finetuning Hyperparameters\n",
    "grid_regressor = ParamGridBuilder() \\\n",
    "        .addGrid(rf_regressor.maxDepth, [5, 15]) \\\n",
    "        .addGrid(rf_regressor.numTrees, [20, 50]) \\\n",
    "        .build()\n",
    "# Create CrossValidator\n",
    "crossval_regressor = CrossValidator(estimator=rf_regressor,\n",
    "                                     estimatorParamMaps=grid_regressor,\n",
    "                                     evaluator=rmse_evaluator,\n",
    "                                     numFolds=3)  # 3-fold cross-validation\n",
    "# Fit the models \n",
    "models = crossval_regressor.fit(training)\n",
    "# Extract the best model that has the best validation\n",
    "model = models.bestModel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
