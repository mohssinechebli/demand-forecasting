{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import mlflow\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import VectorAssembler, Tokenizer, HashingTF, IDF, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder # in case we have compute ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize Spark session\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m my_spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSales_Forecast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "my_spark = SparkSession.builder.appName(\"Sales_Forecast\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing sales data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sales_data \u001b[38;5;241m=\u001b[39m \u001b[43mmy_spark\u001b[49m\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnline Retail.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m                                header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      4\u001b[0m                                inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      5\u001b[0m                                sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Display the schema\u001b[39;00m\n\u001b[1;32m      7\u001b[0m sales_data\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing sales data\n",
    "sales_data = my_spark.read.csv(\"Online Retail.csv\", \n",
    "                               header=True, \n",
    "                               inferSchema=True, \n",
    "                               sep=\",\")\n",
    "# Display the schema\n",
    "sales_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define renaming dictionary\n",
    "renaming_dict = {\n",
    "    \"InvoiceNo\": \"invoice_no\",\n",
    "    \"StockCode\": \"stock_code\",\n",
    "    \"Description\": \"description\",\n",
    "    \"Quantity\": \"quantity\",\n",
    "    \"UnitPrice\": \"unit_price\",\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"Country\": \"country\",\n",
    "    \"InvoiceDate\": \"invoice_date\",\n",
    "    \"Year\": \"year\",\n",
    "    \"Month\": \"month\",\n",
    "    \"Week\": \"week\",\n",
    "    \"Day\": \"day\",\n",
    "    \"DayOfWeek\": \"day_of_week\"\n",
    "}\n",
    "class RenameColumns(Transformer):\n",
    "    def __init__(self, renaming_dict):\n",
    "        super(RenameColumns, self).__init__()\n",
    "        self.renaming_dict = renaming_dict\n",
    "\n",
    "    def _transform(self, dataset: DataFrame) -> DataFrame:\n",
    "        return dataset.select([col(c).alias(self.renaming_dict.get(c, c)) for c in dataset.columns])\n",
    "class MeanEncoder(Transformer):\n",
    "    def __init__(self, inputCol=None, targetCol=None, outputCol=None):\n",
    "        super(MeanEncoder, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.targetCol = targetCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        # Calculate the mean of the target column for each unique value in the input column\n",
    "        encoding_df = df.groupBy(self.inputCol).agg(F.mean(self.targetCol).alias(self.outputCol))\n",
    "        # Join this mean encoding back to the original DataFrame\n",
    "        return df.join(encoding_df, on=self.inputCol, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage 1 : Create an instance of the RenameColumns transformer\n",
    "rename_transformer = RenameColumns(renaming_dict)\n",
    "\n",
    "# stage 2 : Create the country StringIndexer\n",
    "country_indexer = StringIndexer(inputCol=\"country\",\n",
    "                                outputCol=\"country_index\")\n",
    "\n",
    "# stage 3 : Create the country OneHotEncoder\n",
    "country_encoder = OneHotEncoder(inputCol=\"country_index\",\n",
    "                                outputCol=\"country_fact\")\n",
    "\n",
    "# stage 4 : Create the stock_code MeanEncoder\n",
    "mean_encoder = MeanEncoder(inputCol=\"stock_code\", \n",
    "                           targetCol=\"quantity\", \n",
    "                           outputCol=\"stock_code_mean\")\n",
    "\n",
    "# stage 5 : Create the descripotion Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"description\", \n",
    "                      outputCol=\"description_words\")\n",
    "\n",
    "# stage 6 : Create the HashingTF \n",
    "hashing_tf = HashingTF(inputCol=\"description_words\", \n",
    "                       outputCol=\"description_tf\", \n",
    "                       numFeatures=1000)\n",
    "# stage 7 : Create the IDF\n",
    "idf = IDF(inputCol=\"description_tf\", outputCol=\"description_tfidf\")\n",
    "\n",
    "# stage 8 : Create the day_of_week_encoder\n",
    "day_of_week_encoder = OneHotEncoder(inputCols=[\"day_of_week\"], outputCols=[\"day_of_week_encoded\"])\n",
    "\n",
    "# stage 9 : Create the month_encoder\n",
    "month_encoder = OneHotEncoder(inputCols=[\"month\"], outputCols=[\"month_encoded\"])\n",
    "\n",
    "# stage 10 : Make a VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"invoice_no\",\n",
    "                                           \"country_fact\", \n",
    "                                           \"unit_price\", \n",
    "                                           \"customer_id\", \n",
    "                                           \"year\", \n",
    "                                           \"month_encoded\",\n",
    "                                           \"week\",\n",
    "                                           \"day\",\n",
    "                                           \"day_of_week_encoded\",\n",
    "                                           \"stock_code_mean\",\n",
    "                                           \"description_tfidf\"], \n",
    "                                outputCol=\"features\")\n",
    "# Make the pipeline\n",
    "demand_pipe = Pipeline(stages=[rename_transformer,\n",
    "                               country_indexer, \n",
    "                               country_encoder,\n",
    "                               mean_encoder,\n",
    "                               tokenizer,\n",
    "                               hashing_tf,\n",
    "                               idf,\n",
    "                               day_of_week_encoder,\n",
    "                               month_encoder,\n",
    "                               vec_assembler])\n",
    "# Fit and transform your data with the pipeline\n",
    "pipeline_model = demand_pipe.fit(sales_data)\n",
    "transformed_data = pipeline_model.transform(sales_data)\n",
    "\n",
    "# Define the splitting date\n",
    "split_date = \"2011-09-25\"\n",
    "\n",
    "# Split the data into two sets based on the splitting date\n",
    "training = transformed_data.filter(transformed_data.invoice_date <= split_date)\n",
    "test = transformed_data.filter(transformed_data.invoice_date > split_date)\n",
    "\n",
    "# Create the RegressionEvaluator instances\n",
    "rmse_evaluator = RegressionEvaluator(labelCol=\"quantity\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "mean_quantity = sales_data.agg(F.mean(\"Quantity\").alias(\"mean_quantity\")).collect()[0]['mean_quantity']\n",
    "mae_evaluator = RegressionEvaluator(labelCol=\"quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"quantity\", predictionCol=\"prediction\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Experiments\n",
    "mlflow.set_experiment(\"demand_forecasting_random_forest\")\n",
    "# the model parameters\n",
    "params = {\n",
    "    'maxDepth': 20,\n",
    "    'numTrees': 100,\n",
    "}\n",
    "run_name = f\"maxDepth_{params['maxDepth']}_numTrees_{params['numTrees']}\"\n",
    "# Track\n",
    "with mlflow.start_run(run_name=run_name): \n",
    "    # Train\n",
    "    rf_regressor = RandomForestRegressor(featuresCol=\"features\",\n",
    "                                        labelCol=\"quantity\",\n",
    "                                        maxDepth =params[\"maxDepth\"],\n",
    "                                        numTrees=params[\"numTrees\"])  \n",
    "    model = rf_regressor.fit(training)\n",
    "    # Test\n",
    "    test_results = model.transform(test)\n",
    "    rmse = rmse_evaluator.evaluate(test_results)\n",
    "    rrmse = 100*(rmse/mean_quantity)\n",
    "    r2 = r2_evaluator.evaluate(test_results)\n",
    "    # Log \n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"RRMSE\", rrmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    mlflow.spark.log_model(model, \"RandomForestModel\")\n",
    "    mlflow.log_params(params)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# In case we have computational power \\n\\n# Create the parameter grid\\ngrid_regressor = ParamGridBuilder()         .addGrid(rf_regressor.maxDepth, [5, 15])         .addGrid(rf_regressor.numTrees, [20, 50])         .build()\\n# Create CrossValidator\\ncrossval_regressor = CrossValidator(estimator=rf_regressor,\\n                                     estimatorParamMaps=grid_regressor,\\n                                     evaluator=regressor_evaluator,\\n                                     numFolds=3)  # 3-fold cross-validation\\n# Fit the model\\nmodels = crossval_regressor.fit(training)\\n\\n# Extract the best model\\nmodel = models.bestModel\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Finetuning Hyperparameters\n",
    "grid_regressor = ParamGridBuilder() \\\n",
    "        .addGrid(rf_regressor.maxDepth, [5, 15]) \\\n",
    "        .addGrid(rf_regressor.numTrees, [20, 50]) \\\n",
    "        .build()\n",
    "# Create CrossValidator\n",
    "crossval_regressor = CrossValidator(estimator=rf_regressor,\n",
    "                                     estimatorParamMaps=grid_regressor,\n",
    "                                     evaluator=rmse_evaluator,\n",
    "                                     numFolds=3)  # 3-fold cross-validation\n",
    "# Fit the models \n",
    "models = crossval_regressor.fit(training)\n",
    "# Extract the best model that has the best validation\n",
    "model = models.bestModel\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
