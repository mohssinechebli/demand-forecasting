{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import mlflow\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import VectorAssembler, Tokenizer, HashingTF, IDF, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder # in case we have compute ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "my_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: integer (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Week: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importing sales data\n",
    "sales_data = my_spark.read.csv(\"Online Retail.csv\", \n",
    "                               header=True, \n",
    "                               inferSchema=True, \n",
    "                               sep=\",\")\n",
    "# Display the schema\n",
    "sales_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define renaming dictionary\n",
    "renaming_dict = {\n",
    "    \"InvoiceNo\": \"invoice_no\",\n",
    "    \"StockCode\": \"stock_code\",\n",
    "    \"Description\": \"description\",\n",
    "    \"Quantity\": \"quantity\",\n",
    "    \"UnitPrice\": \"unit_price\",\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"Country\": \"country\",\n",
    "    \"InvoiceDate\": \"invoice_date\",\n",
    "    \"Year\": \"year\",\n",
    "    \"Month\": \"month\",\n",
    "    \"Week\": \"week\",\n",
    "    \"Day\": \"day\",\n",
    "    \"DayOfWeek\": \"day_of_week\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenameColumns(Transformer):\n",
    "    def __init__(self, renaming_dict):\n",
    "        super(RenameColumns, self).__init__()\n",
    "        self.renaming_dict = renaming_dict\n",
    "\n",
    "    def _transform(self, dataset: DataFrame) -> DataFrame:\n",
    "        return dataset.select([col(c).alias(self.renaming_dict.get(c, c)) for c in dataset.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEncoder(Transformer):\n",
    "    def __init__(self, inputCol=None, targetCol=None, outputCol=None):\n",
    "        super(MeanEncoder, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.targetCol = targetCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        # Calculate the mean of the target column for each unique value in the input column\n",
    "        encoding_df = df.groupBy(self.inputCol).agg(F.mean(self.targetCol).alias(self.outputCol))\n",
    "        # Join this mean encoding back to the original DataFrame\n",
    "        return df.join(encoding_df, on=self.inputCol, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage 1 : Create an instance of the RenameColumns transformer\n",
    "rename_transformer = RenameColumns(renaming_dict)\n",
    "\n",
    "# stage 2 : Create the country StringIndexer\n",
    "country_indexer = StringIndexer(inputCol=\"country\",\n",
    "                                outputCol=\"country_index\")\n",
    "\n",
    "# stage 3 : Create the country OneHotEncoder\n",
    "country_encoder = OneHotEncoder(inputCol=\"country_index\",\n",
    "                                outputCol=\"country_fact\")\n",
    "\n",
    "# stage 4 : Create the stock_code MeanEncoder\n",
    "mean_encoder = MeanEncoder(inputCol=\"stock_code\", \n",
    "                           targetCol=\"quantity\", \n",
    "                           outputCol=\"stock_code_mean\")\n",
    "\n",
    "# stage 5 : Create the descripotion Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"description\", \n",
    "                      outputCol=\"description_words\")\n",
    "\n",
    "# stage 6 : Create the HashingTF \n",
    "hashing_tf = HashingTF(inputCol=\"description_words\", \n",
    "                       outputCol=\"description_tf\", \n",
    "                       numFeatures=1000)\n",
    "# stage 7 : Create the IDF\n",
    "idf = IDF(inputCol=\"description_tf\", outputCol=\"description_tfidf\")\n",
    "\n",
    "# stage 8 : Create the day_of_week_encoder\n",
    "day_of_week_encoder = OneHotEncoder(inputCols=[\"day_of_week\"], outputCols=[\"day_of_week_encoded\"])\n",
    "\n",
    "# stage 9 : Create the month_encoder\n",
    "month_encoder = OneHotEncoder(inputCols=[\"month\"], outputCols=[\"month_encoded\"])\n",
    "\n",
    "# stage 10 : Make a VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"invoice_no\",\n",
    "                                           \"country_fact\", \n",
    "                                           \"unit_price\", \n",
    "                                           \"customer_id\", \n",
    "                                           \"year\", \n",
    "                                           \"month_encoded\",\n",
    "                                           \"week\",\n",
    "                                           \"day\",\n",
    "                                           \"day_of_week_encoded\",\n",
    "                                           \"stock_code_mean\",\n",
    "                                           \"description_tfidf\"], \n",
    "                                outputCol=\"features\")\n",
    "# Make the pipeline\n",
    "demand_pipe = Pipeline(stages=[rename_transformer,\n",
    "                               country_indexer, \n",
    "                               country_encoder,\n",
    "                               mean_encoder,\n",
    "                               tokenizer,\n",
    "                               hashing_tf,\n",
    "                               idf,\n",
    "                               day_of_week_encoder,\n",
    "                               month_encoder,\n",
    "                               vec_assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RandomForest Estimator\n",
    "rf_regressor = RandomForestRegressor(featuresCol=\"features\", \n",
    "                                     labelCol=\"quantity\",\n",
    "                                     maxDepth =2,\n",
    "                                     numTrees=20)\n",
    "# Create the RegressionEvaluator instances\n",
    "rmse_evaluator = RegressionEvaluator(labelCol=\"quantity\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "mae_evaluator = RegressionEvaluator(labelCol=\"quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"quantity\", predictionCol=\"prediction\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 12:15:07 WARN MemoryStore: Not enough space to cache rdd_189_1 in memory! (computed 28.8 MiB so far)\n",
      "24/10/26 12:15:07 WARN BlockManager: Persisting block rdd_189_1 to disk instead.\n",
      "24/10/26 12:15:09 WARN MemoryStore: Not enough space to cache rdd_189_0 in memory! (computed 152.2 MiB so far)\n",
      "24/10/26 12:15:09 WARN BlockManager: Persisting block rdd_189_0 to disk instead.\n",
      "24/10/26 12:15:12 WARN MemoryStore: Not enough space to cache rdd_189_1 in memory! (computed 348.8 MiB so far)\n",
      "24/10/26 12:15:14 WARN MemoryStore: Not enough space to cache rdd_189_0 in memory! (computed 64.9 MiB so far)\n",
      "24/10/26 12:15:19 WARN MemoryStore: Not enough space to cache rdd_189_0 in memory! (computed 152.2 MiB so far)\n",
      "24/10/26 12:15:19 WARN MemoryStore: Not enough space to cache rdd_189_1 in memory! (computed 232.4 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit and transform your data with the pipeline\n",
    "pipeline_model = demand_pipe.fit(sales_data)\n",
    "transformed_data = pipeline_model.transform(sales_data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "training, test = transformed_data.randomSplit([.8, .2])\n",
    "\n",
    "\"\"\"\n",
    "# In case we have computational power \n",
    "\n",
    "# Create the parameter grid\n",
    "grid_regressor = ParamGridBuilder() \\\n",
    "        .addGrid(rf_regressor.maxDepth, [5, 15]) \\\n",
    "        .addGrid(rf_regressor.numTrees, [20, 50]) \\\n",
    "        .build()\n",
    "# Create CrossValidator\n",
    "crossval_regressor = CrossValidator(estimator=rf_regressor,\n",
    "                                     estimatorParamMaps=grid_regressor,\n",
    "                                     evaluator=regressor_evaluator,\n",
    "                                     numFolds=3)  # 3-fold cross-validation\n",
    "# Fit the model\n",
    "models = crossval_regressor.fit(training)\n",
    "\n",
    "# Extract the best model\n",
    "model = models.bestModel\n",
    "\"\"\"\n",
    "\n",
    "# Fit the model\n",
    "model = rf_regressor.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 123:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 8.4400017516277\n",
      "MAE: 5.855641517677259\n",
      "R²: 0.19130013266924561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Scale of rmse: 101.6082768076977\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = model.transform(test)\n",
    "\n",
    "# Calculate each metric\n",
    "mean_quantity = sales_data.agg(F.mean(\"Quantity\").alias(\"mean_quantity\")).collect()[0]['mean_quantity']\n",
    "rmse = rmse_evaluator.evaluate(test_results)\n",
    "rrmse = 100*(rmse/mean_quantity)\n",
    "r2 = r2_evaluator.evaluate(test_results)\n",
    "\n",
    "# Print the results\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"RRMSE: {rrmse}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 12:55:20 WARN MemoryStore: Not enough space to cache rdd_1143_1 in memory! (computed 44.4 MiB so far)\n",
      "24/10/26 12:55:20 WARN BlockManager: Persisting block rdd_1143_1 to disk instead.\n",
      "24/10/26 12:55:22 WARN MemoryStore: Not enough space to cache rdd_1143_0 in memory! (computed 156.4 MiB so far)\n",
      "24/10/26 12:55:22 WARN BlockManager: Persisting block rdd_1143_0 to disk instead.\n",
      "24/10/26 12:55:26 WARN MemoryStore: Not enough space to cache rdd_1143_1 in memory! (computed 358.2 MiB so far)\n",
      "24/10/26 12:55:28 WARN MemoryStore: Not enough space to cache rdd_1143_0 in memory! (computed 66.6 MiB so far)\n",
      "24/10/26 12:55:43 WARN MemoryStore: Not enough space to cache rdd_1143_1 in memory! (computed 156.4 MiB so far)\n",
      "24/10/26 12:55:44 WARN MemoryStore: Not enough space to cache rdd_1143_0 in memory! (computed 238.7 MiB so far)\n",
      "24/10/26 12:56:01 WARN MemoryStore: Not enough space to cache rdd_1143_0 in memory! (computed 156.4 MiB so far)\n",
      "24/10/26 12:56:02 WARN MemoryStore: Not enough space to cache rdd_1143_1 in memory! (computed 238.7 MiB so far)\n",
      "24/10/26 12:56:20 WARN MemoryStore: Not enough space to cache rdd_1143_0 in memory! (computed 156.4 MiB so far)\n",
      "24/10/26 12:56:20 WARN MemoryStore: Not enough space to cache rdd_1143_1 in memory! (computed 238.7 MiB so far)\n",
      "24/10/26 12:56:39 WARN DAGScheduler: Broadcasting large task binary with size 1558.5 KiB\n",
      "24/10/26 12:56:39 WARN MemoryStore: Not enough space to cache rdd_1143_1 in memory! (computed 156.4 MiB so far)\n",
      "24/10/26 12:56:40 WARN MemoryStore: Not enough space to cache rdd_1143_0 in memory! (computed 238.7 MiB so far)\n",
      "2024/10/26 12:57:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"demand_forecasting_random_forest\")\n",
    "\n",
    "# the model parameters\n",
    "params = {\n",
    "    'maxDepth': 5,\n",
    "    'numTrees': 50,\n",
    "}\n",
    "run_name = f\"maxDepth_{params['maxDepth']}_numTrees_{params['numTrees']}\"\n",
    "\n",
    "# Start MLflow tracking\n",
    "with mlflow.start_run(run_name=run_name): \n",
    "    rf_regressor = RandomForestRegressor(featuresCol=\"features\",\n",
    "                                        labelCol=\"quantity\",\n",
    "                                        maxDepth =params[\"maxDepth\"],\n",
    "                                        numTrees=params[\"numTrees\"])  \n",
    "    model = rf_regressor.fit(training)\n",
    "    test_results = model.transform(test)\n",
    "    rmse = rmse_evaluator.evaluate(test_results)\n",
    "    rrmse = 100*(rmse/mean_quantity)\n",
    "    r2 = r2_evaluator.evaluate(test_results)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"RRMSE\", rrmse)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    # Log model\n",
    "    mlflow.spark.log_model(model, \"RandomForestModel\")\n",
    "    # Log parameters\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "# End the run\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
